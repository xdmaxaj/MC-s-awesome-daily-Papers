# MC-s-awesome-daily-Papers
ðŸ“š Daily English Literature Reading List                                                                              
A curated collection of English academic papers, articles, and technical documents I read daily. This repository        serves as my personal knowledge repository, organizing interesting research across machine learning, artificial
intelligence, computer science, and related fields.

ðŸ“– Contents

- Research Papers - arXiv preprints, conference papers (NeurIPS, ICML, ICLR, etc.)
- Technical Articles - Deep dives into algorithms, system design, and best practices
- Blog Posts - Insights from researchers and industry practitioners
- Notes & Summaries - Key takeaways and personal annotations

ðŸ—‚ï¸ Organization

Papers are organized by:
- Research area (NLP, Computer Vision, Systems, etc.)
- Publication date
- Reading status (to-read, reading, completed)

ðŸ“ Format

Each entry includes:
- Paper/article title and link
- Authors/source
- Brief summary of key contributions
- Personal notes and references

ðŸŽ¯ Purpose

- Track reading progress and build a searchable reference library
- Share interesting findings with the research community
- Maintain continuity in learning across different domains
- Document the evolution of ideas in AI/ML research

ðŸ’¡ Contributing

Feel free to suggest papers and articles via issues or pull requests!
# Collections
## A. Safety
### A0. General
**A0.1** UpSafeC Upcycling for Controllable Safety in Large Language Models--[[Paper]()]--[[Repo]()]
> Large Language Models (LLMs) have achieved remarkable progress across a wide range of tasks, but remain vulnerable to safety risks such as harmful content generation and jailbreak attacks. Existing...

**A0.2** Unravelling the Mechanisms of Manipulating Numbers in Language Models--[[Paper]()]--[[Repo]()]
> Recent work has shown that different large language models (LLMs) converge to similar and accurate input embedding representations for numbers. These findings conflict with the documented propensity...

**A0.3** Interpretation Meets Safety A Survey on Interpretation Methods and Tools for Improving LLM Safety--[2025]--[[Paper]()]--[[Repo]()]
> As large language models (LLMs) see wider real-world use, understanding and mitigating their unsafe behaviors is critical. Interpreta- tion techniques can reveal causes of unsafe out- puts and guide...

**A0.4** Finding and Reactivating Post-Trained LLMs' Hidden Safety Mechanisms--[2025]--[[Paper]()]--[[Repo]()]
> Despite the impressive performance of general-purpose large language models (LLMs), they often require fine-tuning or post-training to excel at specific tasks. For instance, large reasoning models...

**A0.5** Beyond the Black Box Theory and Mechanism of Large Language Models--[[Paper]()]--[[Repo]()]
> The rapid emergence of Large Language Models (LLMs) has precipitated a pro- found paradigm shift in Artificial Intelligence, delivering monumental engineering successes that increasingly impact...

**A0.6** Safety at One Shot Patching Fine-Tuned LLMs with A Single Instance--[[Paper]()]--[[Repo]()]
> Fine-tuning safety-aligned large language models (LLMs) can substantially com- promise their safety. Previous approaches require many safety samples or cali- bration sets, which not only incur...

### A1. Jailbreak 
**A1.1** Fewer Weights More Problems A Practical Attack on LLM Pruning--[[Paper]()]--[[Repo]()]
> Model pruning, i.e., removing a subset of model weights, has become a prominent approach to reducing the memory footprint of large language models (LLMs) during inference. Notably, popular inference...

### A2. Alignment 
**A2.1** LatentGuard Controllable Latent Steering for Robust Refusal of Attacks and Reliable Response Generation--[[Paper]()]--[[Repo]()]
> Achieving robust safety alignment in large language models (LLMs) while pre- serving their utility remains a fundamental challenge. Existing approaches often struggle to balance comprehensive safety...

**A2.2** What makes and breaks safety fine-tuning a mechanistic study--[2024]--[[Paper]()]--[[Repo]()]
> Safety fine-tuning helps align Large Language Models (LLMs) with human pref- erences for their safe deployment. To better understand the underlying factors that make models safe via safety...

**A2.3** Any-Depth Alignment Unlocking Innate Safety Alignment of LLMs to Any-Depth--[[Paper]()]--[[Repo]()]
> Large Language Models (LLMs) exhibit strong but shallow alignment: they directly refuse harmful queries when a refusal is expected at the very start of an assistant turn, yet this protection...

**A2.4** Wisdom is Knowing What not to Say Hallucination-Free LLMs Unlearning via Attention Shifting--[[Paper]()]--[[Repo]()]
> The increase in computing power and the necessity of AI-assisted decision-making boost the growing application of Large Language Models (LLMs). Along with this, the potential retention of sensitive...

**A2.5** SABER Uncovering Vulnerabilities in Safety Alignment via Cross-Layer Residual Connection--[2025]--[[Paper]()]--[[Repo]()]
> Large Language Models (LLMs) with safe- alignment training are powerful instruments with robust language comprehension capabil- ities. These models typically undergo metic- ulous alignment procedures...

**A2.6** Attention Eclipse Manipulating Attention to Bypass LLM Safety-Alignment--[2025]--[[Paper]()]--[[Repo]()]
> Recent research has shown that carefully crafted jailbreak inputs can induce large lan- guage models to produce harmful outputs, de- spite safety measures such as alignment. It is important to...

**A2.7** Understanding and Mitigating Overrefusal in LLMs from an Unveiling Perspective of Safety Decision Boundary--[2025]--[[Paper]()]--[[Repo]()]
> Large language models (LLMs) have demon- strated remarkable capabilities across a wide range of tasks, yet they often refuse to answer legitimate queriesâ€”a phenomenon known as overrefusal....

**A2.8** Automating Steering for Safe Multimodal Large Language Models--[2025]--[[Paper]()]--[[Repo]()]
> Recent progress in Multimodal Large Lan- guage Models (MLLMs) has unlocked pow- erful cross-modal reasoning abilities, but also raised new safety concerns, particularly when faced with adversarial...

**A2.9** Towards Understanding Safety Alignment A Mechanistic Perspective from Safety Neurons--[[Paper]()]--[[Repo]()]
> Large language models (LLMs) excel in various capabilities but pose safety risks such as generating harmful content and misinformation, even after safety align- ment. In this paper, we explore the...

**A2.10** AlphaSteer Learning Refusal Steering with Principled Null-Space Constraint--[[Paper]()]--[[Repo]()]
> As LLMs are increasingly deployed in real-world applications, ensuring their abil- ity to refuse malicious prompts, especially jailbreak attacks, is essential for safe and reliable use. Recently,...

### A3. Deepfake 


### A4. Ethics
### A5. Fairness
### A6. Hallucination
**A6.1** Training-free Truthfulness Detection via Value Vectors in LLMs--[[Paper]()]--[[Repo]()]
> Large language models often generate factually incorrect outputs, motivating efforts to detect the truthfulness of their content. Most exist- ing approaches rely on training probes over internal...

**A6.2** Lookback lens Detecting and mitigating contextual hallucinations in large language models using only attention maps--[2024]--[[Paper]()]--[[Repo]()]
> When asked to summarize articles or answer questions given a passage, large language mod- els (LLMs) can hallucinate details and respond with unsubstantiated answers that are inaccu- rate with...

**A6.3** Cram Credibility-aware attention modification in llms for combating misinformation in rag--[2025]--[[Paper]()]--[[Repo]()]
> Retrieval-Augmented Generation (RAG) can alleviate hallu- cinations of Large Language Models (LLMs) by referenc- ing external documents. However, the misinformation in ex- ternal documents may...

**A6.4** Fact-Checking with Large Language Models via Probabilistic Certainty and Consistency--[[Paper]()]--[[Repo]()]
> Large language models (LLMs) are increasingly used in applications requiring factual accuracy, yet their outputs often contain hallucinated responses. While fact-checking can mitigate these errors,...

### A7. Prompt Injection
### A8. Toxicity

## B. Security
### B0. General
### B1. Adversarial Examples
### B2. Agent
### B3. Poison & Backdoor
### B4. Side-Channel
### B5. System

## C. Privacy
### C0. General
### C1. Contamination
### C2. Data Reconstruction
### C3. Membership Inference Attacks
### C4. Model Extraction
### C5. Privacy-Preserving Computation
### C6. Property Inference Attacks
### C7. Side-Channel
### C8. Unlearning
**C8.1** Leverage Unlearning to Sanitize LLMs--[2025]--[[Paper]()]--[[Repo]()]
> Pre-trained large language models (LLMs) are becoming useful for various tasks. To improve their performance on certain tasks, it is necessary to fine-tune them on specific data corpora (e.g., med-...

### C9. Watermark & Copyright

## D. Interpretability
### D0. Survey
**D0.1** Gemma Open Models Based on Gemini Research and Technology--[2024]--[[Paper]()]--[[Repo]()]

**D0.2** Gemma 3 Technical Report--[2025]--[[Paper]()]--[[Repo]()]

**D0.3** Survey of Different Large Language Model Architectures Trends Benchmarks and Challenges--[2024]--[[Paper]()]--[[Repo]()]
> Large Language Models (LLMs) represent a class of deep learning models adept at understanding natural language and generating coherent responses to various prompts or queries. These models far exceed...

**D0.4** Evolution and Optimization of Language Model Architectures From Foundations to Future Directions--[2025]--[[Paper]()]--[[Repo]()]
> In the continuously advancing domain of artiï¬cial intelligence, language model architectures have undergone a signiï¬cant transformation, evolving from fun-damental statistical methods to...

**D0.5** The evolution of mixture of experts A survey from basics to breakthroughs--[2024]--[[Paper]()]--[[Repo]()]
> The Mixture of Experts (MoE) architecture has evolved as a powerful and versatile approach for improving the performance and efficiency of deep learning models. This survey aims to provide the...

### D1. Interpretability Safety

**D1.1** Attention Consistency for LLMs Explanation--[[Paper]()]--[[Repo]()]
> Understanding the decision-making processes of large language models (LLMs) is essential for their trustworthy development and deploy- ment. However, current interpretability meth- ods often face...

**D1.2** Interpreting LLM-as-a-Judge Policies via Verifiable Global Explanations--[[Paper]()]--[[Repo]()]
> Using LLMs to evaluate text, that is, LLM-as-a-judge, is increasingly being used at scale to augment or even replace human annotations. As such, it is imperative that we understand the potential...

**D1.3** Transcoders Find Interpretable LLM Feature Circuits--[2024]--[[Paper]()]--[[Repo]()]
> A key goal in mechanistic interpretability is circuit analysis: finding sparse sub- graphs of models corresponding to specific behaviors or capabilities. However, MLP sublayers make fine-grained...

**D1.4** Towards Unified Attribution in Explainable AI, Data-Centric AI, and Mechanistic Interpretability--[[Paper]()]--[[Repo]()]
> The increasing complexity of AI systems has made understanding their behavior critical. Numer- ous interpretability methods have been developed to attribute model behavior to three key aspects: input...

**D1.5** Grad-ELLM Gradient-based Explanations for Decoder-only LLMs--[[Paper]()]--[[Repo]()]
> Large Language Models (LLMs) have demon- strated remarkable capabilities across diverse tasks, yet their black-box nature raises concerns about transparency and faithfulness. Input at- tribution...

**D0.1** Locate, Steer, and Improve: A Practical Survey of Actionable Mechanistic Interpretability in Large Language Models--[20 Jan 2026]--[[Paper](https://arxiv.org/abs/2601.14004)]--[[Repo](https://github.com/rattlesnakey/Awesome-Actionable-MI-Survey)]
> Mechanistic Interpretability (MI) has emerged as a vital approach to demystify the opaque decision-making of Large Language Models (LLMs). However, existing reviews primarily treat MI as an...

**D1.7** AlphaSteer: Learning Refusal Steering with Principled Null-Space Constraint--[ICLR 2026]--[20 Jan 2026]--[[Paper](https://arxiv.org/abs/2506.07022)]--[[Repo](https://github.com/AlphaLab-USTC/AlphaSteer)]
> As LLMs are increasingly deployed in real-world applications, ensuring their abil- ity to refuse malicious prompts, especially jailbreak attacks, is essential for safe and reliable use. Recently,...

### D2. Pruning & Compression

**D2.1** A Survey on Sparse Autoencoders Interpreting the Internal Mechanisms of Large Language Models--[[Paper]()]--[[Repo]()]
> Large Language Models (LLMs) have trans- formed natural language processing, yet their internal mechanisms remain largely opaque. Recently, mechanistic interpretability has at- tracted significant...

**D2.2** Scaling and evaluating sparse autoencoders--[2024]--[[Paper]()]--[[Repo]()]
> Sparse autoencoders provide a promising unsupervised approach for extracting in- terpretable features from a language model by reconstructing activations from a sparse bottleneck layer. Since...

**D2.3** Wanda++ Pruning Large Language Models via Regional Gradients--[[Paper]()]--[[Repo]()]
> Large Language Models (LLMs) pruning seeks to remove unimportant weights for in- ference speedup with minimal accuracy im- pact. However, existing methods often suf- fer from accuracy degradation...

**D2.4** A Simple and Effective Pruning Approach for Large Language Models--[2024]--[[Paper]()]--[[Repo]()]
> As their size increases, Large Languages Models (LLMs) are natural candidates for network pruning methods: approaches that drop a subset of network weights while striving to preserve performance....

### D3. Attention & Mechanisms

**D3.1** Inverse-Free Wilson Loops for Transformers A Practical Diagnostic for Invariance and Order Sensitivity--[[Paper]()]--[[Repo]()]
> Large language models can change answers under harmless edits that matter in practice: RAG outputs flip when passages are reordered, fine-tuning erodes invariances learned at pre- training, debate or...

**D3.2** On the role of attention heads in large language model safety--[2025]--[[Paper]()]--[[Repo]()]
> Large language models (LLMs) achieve state-of-the-art performance on multiple language tasks, yet their safety guardrails can be circumvented, leading to harmful generations. In light of this, recent...

**D3.3** Gated Attention for Large Language Models Non-linearity, Sparsity, and Attention-Sink-Free--[[Paper]()]--[[Repo]()]
> Gatingmechanismshavebeenwidelyutilized,fromearlymodelslikeLSTMs(Hochreiter & Schmidhuber, 1997) and Highway Networks (Srivastava et al., 2015) to recent state space models (Gu & Dao, 2023), linear...

**D3.4** For Better or for Worse, Transformers Seek Patterns for Memorization--[2025]--[[Paper]()]--[[Repo]()]
> Memorization in language models is a critical yet poorly understood phenomenon. In this work, we investigate memorization in transformer-based language models by analyzing their memorization dynamics...

**D3.5** The Atlas of In-Context Learning How Attention Heads Shape In-Context Retrieval Augmentation--[[Paper]()]--[[Repo]()]
> Large language models are able to exploit in-context learning to access exter- nal knowledge beyond their training data through retrieval-augmentation. While promising, its inner workings remain...

**D3.6** Detecting Hallucinations in Graph Retrieval-Augmented Generation via Attention Patterns and Semantic Alignment--[[Paper]()]--[[Repo]()]
> Graph-based Retrieval-Augmented Generation (GraphRAG) enhances Large Language Models (LLMs) by incorporating external knowledge from linearized subgraphs retrieved from knowledge graphs. However,...

**D3.7** Training-free Context-adaptive Attention for Efficient Long Context Modeling--[[Paper]()]--[[Repo]()]
> â€”Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of natural language processing tasks. These capabilities stem primarily from the self-attention mechanism,...

**D3.8** AttnLRP Attention-aware layer-wise relevance propagation for transformers--[2024]--[[Paper]()]--[[Repo]()]
> Large Language Models are prone to biased predictions and hallucinations, underlining the paramount importance of understanding their model-internal reasoning process. However, achieving faithful...

**D3.9** One Head to Rule Them All Amplifying LVLM Safety through a Single Critical Attention Head--[2025]--[[Paper]()]--[[Repo]()]
> Large Vision-Language Models (LVLMs) have demonstrated impressive capabili- ties in tasks requiring multimodal understanding. However, recent studies indicate that LVLMs are more vulnerable than LLMs...

**D3.10** The Bayesian Geometry of Transformer Attention--[2025]--[[Paper]()]--[[Repo]()]

**D3.11** Gradient Dynamics of Attention How Cross-Entropy Sculpts Bayesian Manifolds(1)--[2025]--[[Paper]()]--[[Repo]()]

**D3.12** Lil Less is Less When Applying Post-Training Sparse-Attention Algorithms in Long-Decode Stage--[[Paper]()]--[[Repo]()]
> Large language models (LLMs) demonstrate strong capabilities across a wide range of complex tasks and are increasingly deployed at scale, placing signiï¬cant demands on in- ference efï¬ciency. Prior...

### D4. Reasoning & Inference

**D4.1** Self-Reflective Generation at Test Time--[[Paper]()]--[[Repo]()]
> Large language models (LLMs) increasingly solve complex reasoning tasks via long chain-of-thought, but their forward-only autoregressive generation process is fragile; early token errors can cascade,...

**D4.2** Demystifying the Roles of LLM Layers in Retrieval, Knowledge, and Reasoning--[[Paper]()]--[[Repo]()]
> Recent studies suggest that the deeper layers of Large Language Models (LLMs) contribute little to representation learning and can often be removed without significant performance loss. However, such...

**D4.3** Enhancing Large Language Model Reasoning via Selective Critical Token Fine-Tuning--[[Paper]()]--[[Repo]()]
> Large language models (LLMs) primarily rely on supervised fine-tuning (SFT) as a key method to adapt pre-trained models to domain-specific tasks such as mathematical reasoning. However, standard SFT...

**D4.4** A Theoretical Study on Bridging Internal Probability and Self-Consistency for LLM Reasoning--[[Paper]()]--[[Repo]()]
> Test-time scaling seeks to improve the reasoning performance of large language models (LLMs) by adding computational resources. A prevalent approach within the field issampling-based test-time...

**D4.5** CircuitSeer Mining High-Quality Data by Probing Mathematical Reasoning Circuits in LLMs--[[Paper]()]--[[Repo]()]
> Large language models (LLMs) have demon- strated impressive reasoning capabilities, but scaling their performance often relies on mas- sive reasoning datasets that are computation- ally expensive to...

**D4.6** A Theoretical Study on Bridging Internal Probability and Self-Consistency for LLM Reasoning--[[Paper]()]--[[Repo]()]
> Test-time scaling seeks to improve the reasoning performance of large language models (LLMs) by adding computational resources. A prevalent approach within the field issampling-based test-time...

**D4.7** DiffAdapt Difficulty-Adaptive Reasoning for Token-Efficient LLM Inference--[[Paper]()]--[[Repo]()]
> Recent reasoning Large Language Models (LLMs) demonstrate remarkable problem-solving abilities but often generate long thinking traces whose utility is unclear. Our work aims to improve their...

**D4.8** Verifying Large Language Models' Reasoning Paths via Correlation Matrix Rank--[[Paper]()]--[[Repo]()]
> Despite the strong reasoning ability of large language models (LLMs), they are prone to errors and hallucinations. As a result, how to check their outputs effec- tively and efficiently has become a...

**D4.9** Understanding How Value Neurons Shape the Generation of Specified Values in LLMs--[2025]--[[Paper]()]--[[Repo]()]
> Rapid integration of large language models (LLMs) into societal applications has intensi- fied concerns about their alignment with univer- sal ethical principles, as their internal value rep-...

**D4.10** Path Drift in Large Reasoning Models How First-Person Commitments Override Safety--[2025]--[[Paper]()]--[[Repo]()]
> As large reasoning models are increasingly de- ployed for complex reasoning tasks, Chain- of-Thought prompting has emerged as a key paradigm for structured inference. Despite early-stage safeguards...

**D4.11** Explaining How Transformers Use Context to Build Predictions--[2023]--[[Paper]()]--[[Repo]()]
> Language Generation Models produce words based on the previous context. Although exist- ing methods offer input attributions as explana- tions for a modelâ€™s prediction, it is still unclear how prior...

**D4.12** Do LLMs Encode Functional Importance of Reasoning Tokens--[[Paper]()]--[[Repo]()]
> Large language models solve complex tasks by generating long reasoning chains, achieving higher accuracy at the cost of increased compu- tational cost and reduced ability to isolate func- tionally...

**D4.13** Entropy-Aligned Decoding of LMs for Better Writing and Reasoning--[[Paper]()]--[[Repo]()]
> Language models (LMs) are trained on billions of tokens in an attempt to recover the true language distribution. Still, vanilla random sampling from LMs yields low quality generations.Decoding...

**D4.14** Intention Collapse Intention-Level Metrics for Reasoning in Language Models--[[Paper]()]--[[Repo]()]
> Every act of language generation involves an irreversible reduction from a rich internal state to a single sequence of tokens. We call this pro- cessintention collapse: a manyâ€“toâ€“one projection from...

**D4.15** ATLAS Adaptive Test-Time Latent Steering with External Verifiers for Enhancing LLMs Reasoning--[[Paper]()]--[[Repo]()]
> Recent work on activation and latent steering has demonstrated that modifying internal rep- resentations can effectively guide large lan- guage models (LLMs) toward improved rea- soning and...

